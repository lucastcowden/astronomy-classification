{"cells":[{"cell_type":"markdown","metadata":{"id":"k0gf5PNiZ9U1"},"source":["#Objective###\n","Given inputs, we want to find out if an observation is a star, galaxy, or quasar."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xRqB92-bCS-"},"outputs":[],"source":["import os\n","import sys\n","sys.path.append(os.path.abspath(\"./util/\"))\n","from csv_data import SkyServerBinaryDatasetWrapper\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"JqhiC5WNIs--"},"source":["##Data##"]},{"cell_type":"markdown","metadata":{"id":"NT1_1c9DimTh"},"source":["###Data Loading###"]},{"cell_type":"markdown","metadata":{"id":"_XZgUdVOgADI"},"source":["Loading in Data using csv_data.py:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3157,"status":"ok","timestamp":1711054616258,"user":{"displayName":"Lucas Cowden","userId":"00723183863400517056"},"user_tz":240},"id":"ggTRsh2cbit1","outputId":"fdcf76c1-055d-43d8-b16a-fb96a0e27436"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/u/0/uc?id=1gYwg5YyaV3zUX-07bLCol8E0M-uX__zX&export=download\n","To: /content/drive/My Drive/MLProjects/Astronomy Classification/util/skyserver.csv\n","100%|██████████| 1.39M/1.39M [00:00<00:00, 54.5MB/s]\n"]}],"source":["# Loading data, the wrapper handles details of loading and data processing\n","wrapper = SkyServerBinaryDatasetWrapper()\n","[train_x, train_y], [valid_x, valid_y], [test_x, test_y] = wrapper.get_flat_datasets()"]},{"cell_type":"markdown","metadata":{"id":"aycLJMnZipLI"},"source":["###Examining Data###"]},{"cell_type":"markdown","metadata":{"id":"Z2dGknLLgMqd"},"source":["Examining the shape of training data inputs (train_x), plus the first 5 samples:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1711054618666,"user":{"displayName":"Lucas Cowden","userId":"00723183863400517056"},"user_tz":240},"id":"xxvqt-QAgRen","outputId":"309ed782-fdac-4de0-f2a3-240e0f9b626a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(7000, 13)\n"]},{"data":{"text/plain":["array([[ 0.16745842, -0.58492272,  1.03148637, -0.34855938, -0.83728027,\n","        -0.94605772, -0.99534154, -0.83806089,  0.21085172, -0.21763043,\n","        -0.36973112,  1.03148936,  1.30931064],\n","       [ 0.16886159, -0.58311429,  0.05243046, -0.16653251, -0.15415531,\n","        -0.08264457, -0.02604308, -0.83806089,  0.21085172, -0.21763043,\n","        -0.36984929, -0.63621258, -0.87919741],\n","       [ 0.17057433, -0.58347525,  0.92156796,  0.86709322,  0.59315368,\n","         0.44120145,  0.31452753, -0.83806089,  0.21085172, -0.21147922,\n","        -0.05302706, -0.65633905, -0.60919097],\n","       [ 0.17455754, -0.58650069, -1.03063038, -0.81362749, -0.63669227,\n","        -0.52660429, -0.43092107, -0.83806089,  0.21085172, -0.20532801,\n","        -0.36999261,  1.03148936,  1.30931064],\n","       [ 0.17482457, -0.58441247, -1.29023238, -1.17251944, -0.37676237,\n","        -0.02510121,  0.15827647, -0.83806089,  0.21085172, -0.20532801,\n","        -0.36818949,  1.03148936,  1.30931064]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["print(train_x.shape)\n","train_x[:5]"]},{"cell_type":"markdown","metadata":{"id":"DXP2wl0aga-k"},"source":["We can see there are 7000 samples with 13 features. These features have already been standardized for us."]},{"cell_type":"markdown","metadata":{"id":"uz7A6epogh5H"},"source":["Examining our training outputs/targets/ground truths (train_y):"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":223,"status":"ok","timestamp":1711059619296,"user":{"displayName":"Lucas Cowden","userId":"00723183863400517056"},"user_tz":240},"id":"OZG8M3NvguQP","outputId":"9920eb3b-d9f7-45e7-8335-7a39a0215096"},"outputs":[{"data":{"text/plain":["array([[0],\n","       [0],\n","       [1],\n","       [0],\n","       [0]])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["train_y[:5]"]},{"cell_type":"markdown","metadata":{"id":"b0nBaMMGgwea"},"source":["There is a 0 when there is no star, and a 1 when there is a star."]},{"cell_type":"markdown","metadata":{"id":"iv4_3AQlajLU"},"source":["##Method 1: Binary 0-1 Classification##\n","Here, we will determine a single value between 0 and 1 that gives us the probability we think an observation is in a SPECIFIC class. For example, if we are looking at whether an observation is a star, and our output is 0.7, this means the model believes there is a 70% chance the observation is a star."]},{"cell_type":"markdown","metadata":{"id":"483B8mQeiswM"},"source":["###Choosing our NN Functions###"]},{"cell_type":"markdown","metadata":{"id":"JxYIPrYHnYvd"},"source":["####NN Output Activation Function: Sigmoid####"]},{"cell_type":"markdown","metadata":{"id":"DVgWCaOuhK41"},"source":["We want to make our neural network output values between 0 and 1. Normally, output values can be anything on the continuous spectrum, but we can use activation functions to transform these values to a (0,1) range. We will use the sigmoid function to achieve this.\n","\n","$\\sigma=\\frac{1}{1+ e^{-x}}$"]},{"cell_type":"markdown","metadata":{"id":"codfZTOfi335"},"source":["The sigmoid uses this form because the $e$ in the sigmoid and the $log$ in the loss function will cancel out, making the gradient easier to calculate."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OOuozoJ4wgUp"},"outputs":[],"source":["sigmoid = lambda x: 1 / (1 + np.exp(-x))"]},{"cell_type":"markdown","metadata":{"id":"eU4eJxtLnay2"},"source":["####Loss Function: Negative Log Likelihood (NLL)####"]},{"cell_type":"markdown","metadata":{"id":"Z8C_Yw5cjUXR"},"source":["We cannot use Mean Squared Error (MSE) for our loss function in classification.\n","\n","$MSE = (actual - predicted)^2$\n","\n","Since our actual and predicted are constrained between 0 and 1, our MSE loss will be between 0 and 1. This does not allow for a steep gradient when performing gradient descent.\n","\n","Instead, we use negative log likelihood(NLL):\n","\n","$NLL = -(y * log(\\hat{y}) + (1-y) * log(1-\\hat{y}))$\n","\n","**NOTE**: $log = log_e = ln$\n","\n","Basically, we are taking our actual value and multiplying it by the log of our predicted value. Then, we add (1 - actual) * log(1-predicted). Since our actual is going to be either 0 or 1, intuitively one of these terms is going to always be 0.\n","\n","This means we will ultimately get a value of the form $-log(z)$. The $log$ expression tells us what value $e$ needs to be raised to in order to equal $z$, and we multiply by $-1$ to make this value positive. Since $z$ will be between 0 and 1, the log function will always be negative.\n","\n","**Note:** We generally add a small tolerance (e.g. 1e-6) to our predicted value to ensure our $log$ expression does not throw errors ($log(0)$ is undefined)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"crtH45DdxaGI"},"outputs":[],"source":["tol = 1e-6\n","nll = lambda pred, actual: -((actual * np.log(pred + tol)) + ((1-actual) * np.log(1-pred + tol)))\n","# Below is the gradient of the loss function with respect to the output of the NN.\n","# Not going to explain how we calculate it, but it simplifies to (predicted - actual)\n","# thanks to our choice of the sigmoid activation function.\n","nll_grad = lambda pred, actual: pred - actual"]},{"cell_type":"markdown","metadata":{"id":"hvpZfXvRn_Ru"},"source":["###Defining the NN (Binary Classification)###"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MU3P0Nw5oCV8"},"outputs":[],"source":["#import network\n","#import activation\n","from dense import Dense\n","\n","class ClassificationNet():\n","  def __init__(self, output_size=1):\n","    self.layer1= Dense(input_size=13, output_size=25)\n","    # We do not wish to apply ReLU function to the second layer\n","    self.layer2 = Dense(input_size=25, output_size=output_size, activation=False)\n","\n","  def forward(self, x):\n","    x = self.layer1.forward(x)\n","    x = self.layer2.forward(x)\n","    return x\n","\n","  def backward(self, grad, lr):\n","    grad = self.layer2.backward(grad, lr)\n","    self.layer1.backward(grad, lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16985,"status":"ok","timestamp":1711055085099,"user":{"displayName":"Lucas Cowden","userId":"00723183863400517056"},"user_tz":240},"id":"vLZ7zdDGv77H","outputId":"e9854386-c31b-470c-a599-59c05c442c58"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 train loss: 0.2682695827790348\n","Valid loss: 0.2644771902817305\n","Epoch: 10 train loss: 0.10104581465738757\n","Valid loss: 0.12039744183988187\n","Epoch: 20 train loss: 0.07648901768969217\n","Valid loss: 0.0931297334278475\n","Epoch: 30 train loss: 0.06804528798961347\n","Valid loss: 0.09299463791060265\n","Epoch: 40 train loss: 0.06247892193181192\n","Valid loss: 0.09788190123806989\n"]}],"source":["net = ClassificationNet(1)\n","# Hyperparameters\n","lr = 1e-2\n","epochs = 50\n","\n","for epoch in range(epochs):\n","  epoch_loss = 0\n","  for x, target in zip(train_x, train_y):\n","    pred = sigmoid(net.forward(x.reshape(1,-1))) # The reshape turns the data into a column instead of a vector\n","    grad = nll_grad(pred, target)\n","    epoch_loss += nll(pred, target)[0,0]\n","\n","    net.backward(grad, lr)\n","\n","  if epoch % 10 == 0:\n","    print(f\"Epoch: {epoch} train loss: {epoch_loss / len(train_x)}\")\n","    epoch_loss = 0\n","    for x,target in zip(valid_x, valid_y):\n","      pred = sigmoid(net.forward(x.reshape(1, -1)))\n","      epoch_loss += nll(pred, target)[0, 0]\n","    print(f\"Valid loss: {epoch_loss / len(valid_x)}\")"]},{"cell_type":"markdown","metadata":{"id":"P-pUzXei1RgI"},"source":["##Method 2: Multiclass Classification##\n","Here, we will perform classification with multiple targets. We can extend our binary classification to get probabilities for $n$ classes. We need to adjust our target a little bit. One number is enough to differentiate between 2 classes (because it can be 0 or 1), but we need multiple numbers to differentiate between multiple classes."]},{"cell_type":"markdown","metadata":{"id":"MJIFKXRgJZZP"},"source":["###Multiclass Encoding###"]},{"cell_type":"markdown","metadata":{"id":"I6nX50f63qOO"},"source":["Below we create a method that allows us to assign unique encodings for each class via 'One-Hot Encoding'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98zFLe_y1U-a"},"outputs":[],"source":["def encode(target, max_value=3):\n","  # Create a vector which has as many zeros as classes\n","  encoded = np.zeros((1, max_value))\n","  # We will set one of these indices to 1 (this is called One-Hot Encoding)\n","  encoded[0, target] = 1\n","  return encoded"]},{"cell_type":"markdown","metadata":{"id":"-EyIQD543x6s"},"source":["Our encodings are as follows:\n","\n","0 = Star => [[1, 0, 0]]\n","\n","1 = Galaxy => [[0, 1, 0]]\n","\n","2 = Quasar => [[0, 0, 1]]\n","\n","**Note:** In the binary classification phase, we said that a 1 corresponded to a star, and a 0 corresponded to not being a star. We are using the same data, however due to the different context, in this scenario assume a 0 is a star and a 1 is a galaxy. We will not see any 2's as we are reusing the dataset meant for binary classification."]},{"cell_type":"markdown","metadata":{"id":"--Pmmje038q5"},"source":["The output of our neural network will be three numbers, with each number corresponding to the probability that each object is associated with the code for its respective class.\n","\n","**NOTE:** These numbers only become probabilities after we pass the outputs through the activation function discussed below. Before this occurs, the values can be anything (not necessarily bounded between 0 and 1)."]},{"cell_type":"markdown","metadata":{"id":"0htd8X3v9Zf8"},"source":["###Choosing our NN Functions###"]},{"cell_type":"markdown","metadata":{"id":"v2i7swRI-X4o"},"source":["####NN Output Activation Function: Softmax####"]},{"cell_type":"markdown","metadata":{"id":"BbZAZ2SJ4gYe"},"source":["We need two achieve two goals for the outputs of our NN:\n","1. The outputs need to fit into an appropriate range, e.g. (0,1)\n","2. All the outputs need to sum to 1"]},{"cell_type":"markdown","metadata":{"id":"2I3o0Dk55CbD"},"source":["By using the sigmoid function, we could handle requirement #1. However, with three numbers, the sum $\\sum_{k=0}^2 (output_k)$ can be greater than 1. Thus, we need to normalize the values across all three using a function that is **not** Sigmoid."]},{"cell_type":"markdown","metadata":{"id":"YdHrnicI5ejF"},"source":["To normalize the values and keep them between 0 and 1, **instead of using the Sigmoid Function**, we can use a function called the Softmax Function:\n","\n","$\\zeta=\\frac{e^{\\hat{y_{i}}}}{\\sum_{j=0}e^{\\hat{y_{j}}}}$\n","\n","In other words, the softmax function equals $e$ raised to the predicted y, divided by the sum of $e$ raised to all predicted y's.\n","\n","Since we will be getting three numbers as our output from our NN, this is basically saying that for each value $z$ in an output [$z_1$, $z_2$, $z_3$], we apply the softmax function to that value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngMUzLIF6rMb"},"outputs":[],"source":["def softmax(preds):\n","  preds = np.exp(preds)\n","  if len(preds.shape) > 1: # This handles if preds is in a matrix\n","    # Sums across the rows, divides predictions by the sums\n","    # So each row in the matrix will be a separate set of predictions we want to softmax\n","    # The .reshape() will reshape the sum into a one column matrix, allowing us to broadcast across\n","    # all the columns/values of each output\n","    normed = preds / np.sum(preds, axis=1).reshape(-1,1)\n","  else: # This handles if preds is in a vector\n","    normed = preds / np.sum(preds)\n","  return normed"]},{"cell_type":"markdown","metadata":{"id":"v-Ff1P4z6oaE"},"source":["**Why does this make sense?**\n","\n","We could use max() to find which value in the output is greatest, and assign probabilities of 1 for the max value's respective class, and 0's for the rest. For example, if we have an output of [10, 20, 30], we could take the max and use this to compute [0, 0, 1]. However, this does not give us good results to use for gradient descent, as we cannot compute $log(0)$. Instead, softmax pushes the maximum value towards 1, and it pushes the other values to a very small value approaching 0. It also normalizes the values so they add up to 1."]},{"cell_type":"markdown","metadata":{"id":"SFmYPMDO-RxK"},"source":["####Loss Function: Multiclass Negative Log Likelihood (NLL)####"]},{"cell_type":"markdown","metadata":{"id":"qtB35uWZ-lj-"},"source":["We will be feeding into our loss function a vector such as [0.01, 0.01, 0.98], where each value is the probability the observation falls into its respective class."]},{"cell_type":"markdown","metadata":{"id":"aRGhiGkK-4o0"},"source":["The generalized negative log likelihood looks as follows:\n","\n","$$NLL = - \\sum_{i=0} y_{i} \\log p_{i}$$"]},{"cell_type":"markdown","metadata":{"id":"QG6vGJhw_GDL"},"source":["Before, when we did binary classification, we had one number, allowing us to compute for a single state. However, this can also be looked at in another way:\n","Say we want to perform binary classification on whether an observation is a star. Then in our loss function what we are really computing is the probability y is a star and that y is NOT a star. This second piece is how we got the $(1-y) * log(1 - \\hat{y})$ expression in our binary NLL."]},{"cell_type":"markdown","metadata":{"id":"zjGgTgvfAnYs"},"source":["Just like in the binary case, however, the ground truth $y$ will only ever have a single 1 in its vector encoding, meaning while there is a summation, we are only ever computing this $y_i \\log{p_i}$ term for the value in the predicted output corresponding to the correct class."]},{"cell_type":"markdown","metadata":{"id":"GXKmLh-aBfq4"},"source":["Ex: Say our ground truth $y$ for a particular observation is [0,1,0], i.e. a galaxy. If our prediction $\\hat{y}$ is [0.01, 0.9, 0.09], we only end up computing the above term for the value 0.9 because the others are negated. So we are only looking at the loss against the ACTUAL $y$ value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dcRovbpClpV"},"outputs":[],"source":["def multiclass_loss(predicted, actual):\n","  tol = 1e-6\n","  # Cross Entropy is essentially how similar one distribution is to another\n","  cross_entropy = actual * np.log(predicted + tol)\n","  return -np.sum(cross_entropy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-r1fP1cPDWX0"},"outputs":[],"source":["# Not going to explain how this is derived, but what happened with our binary NLL and sigmoid also\n","# occurs between our multiclass NLL and the softmax function\n","multiclass_loss_grad = lambda pred, actual: pred - actual"]},{"cell_type":"markdown","metadata":{"id":"iJ9-9EqgCkT6"},"source":["###Defining the NN (Multiclass Classification)###\n","\n","We can actually reuse the ClassificationNet from before, except this time we want to specify our number of outputs to be 3, one for each class. Additionally, in our training loop we want to encode the targets (y values), and we want to use the softmax, multiclass_loss, and multiclass_loss_grad functions.\n","\n","**NOTE:** Because we are using the data for the binary classification, each target (actual y value)will map to 0 or 1, meaning when we encode, we will get [1, 0, 0] or [0, 1, 0]. We will not have any [0, 0, 1] values in our data set, unfortunately."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53161,"status":"ok","timestamp":1711059776368,"user":{"displayName":"Lucas Cowden","userId":"00723183863400517056"},"user_tz":240},"id":"aVDVsja58ZPL","outputId":"c21e4b6e-7b36-410e-d588-511960213549"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 0 train loss: 0.4525173340362456\n","Valid loss: 0.3038497893177284\n","Epoch: 10 train loss: 0.20493730457841222\n","Valid loss: 0.23215598386580852\n","Epoch: 20 train loss: 0.16465595194764043\n","Valid loss: 0.1957477666502035\n","Epoch: 30 train loss: 0.13581549486473626\n","Valid loss: 0.16032609837084882\n","Epoch: 40 train loss: 0.1172396942026143\n","Valid loss: 0.14132894594073564\n"]}],"source":["net = ClassificationNet(3)\n","# Hyperparameters\n","lr = 1e-3\n","epochs = 50\n","\n","for epoch in range(epochs):\n","  epoch_loss = 0\n","  for x, target in zip(train_x, train_y):\n","    pred = softmax(net.forward(x.reshape(1,-1))) # The reshape turns the data into a column instead of a vector\n","    encoded = encode(target)\n","    grad = multiclass_loss_grad(pred, encoded)\n","    epoch_loss += multiclass_loss(pred, encoded)\n","\n","    net.backward(grad, lr)\n","\n","  if epoch % 10 == 0:\n","    print(f\"Epoch: {epoch} train loss: {epoch_loss / len(train_x)}\")\n","    epoch_loss = 0\n","    for x,target in zip(valid_x, valid_y):\n","      pred = softmax(net.forward(x.reshape(1, -1)))\n","      encoded = encode(target)\n","      epoch_loss += multiclass_loss(pred, encoded)\n","    print(f\"Valid loss: {epoch_loss / len(valid_x)}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNXdaXvpvO4eE0oWIDCMFuJ","mount_file_id":"1SxBbd4gPkTiavfFqc2v8j62QOiONXFj9","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
